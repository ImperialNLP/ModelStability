{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in ['sst', '20News_sports', 'tweet', 'AgNews'] :\n",
    "    %run train_and_run_experiments_bc.py --dataset {dataset} --data_dir . --output_dir outputs_dev/ --attention all --encoder all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Trainers.DatasetBC import *\n",
    "from ExperimentsBC import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Trainers.DatasetQA import *\n",
    "from ExperimentsQA import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in datasets :\n",
    "    print(k)\n",
    "#     if k == 'Anemia' :\n",
    "    dataset = datasets[k]()\n",
    "    dataset.basepath = 'outputs_dev'\n",
    "    generate_graphs_on_encoders(dataset, ['cnn', 'lstm', 'average', 'cnn_dot', 'lstm_dot', 'average_dot'])\n",
    "    print('+'*700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets_ehr['mortality']()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Trainers.PlottingBC import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = run_evaluator_on_latest_model(dataset, config='lstm')\n",
    "logodds_results = pload(evaluator.model, 'logodds_attention')\n",
    "emax_jds, emax_adv_attn, emax_ad_y = plot_attn_diff(dataset, dataset.test_data, logodds_results, \n",
    "                                                    save_name='logodds_subs', dirname=evaluator.model.dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 308\n",
    "a = print_adversarial_example(dataset.vec.map2words(dataset.test_data.X[n]), dataset.test_data.attn_hat[n], emax_adv_attn[n], latex=True)\n",
    "dataset.test_data.yt_hat[n], emax_ad_y[n]\n",
    "print(a[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent = [np.max(x) for x in emax_adv_attn]\n",
    "plt.scatter(ent, emax_jds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_1 = set(np.where(np.array(ent) > 0.6)[0]) & set(np.where(np.array(emax_jds) > 0.4)[0])\n",
    "idx_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset(dataset, 'logodds_lstm_reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logodds_experiment(dataset, 'logodds_lstm_reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments_on_latest_model(dataset, 'logodds_lstm_reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_graphs_on_latest_model(dataset, 'logodds_lstm_reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logodds_substitution_experiment(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dataset.test_data\n",
    "plt.scatter(test_data.yt_hat[:, 0], test_data.opp_yt_hat[:, 0], s=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = np.abs(test_data.yt_hat[:, 0] - test_data.opp_yt_hat[:, 0])\n",
    "np.argsort(diff)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(diff, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dataset.test_data\n",
    "for k, v in test_data.logodds_combined[0].items() :\n",
    "    print(dataset.vec.map2words(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4231\n",
    "true_X = dataset.vec.map2words(test_data.X[n])\n",
    "new_X = dataset.vec.map2words(test_data.opp_X[n])\n",
    "print_attn(true_X, dataset.test_data.attn_hat[n])\n",
    "print(test_data.yt_hat[n])\n",
    "print_attn(new_X, dataset.test_data.opp_attn[n])\n",
    "print(test_data.opp_yt_hat[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in datasets_ehr :\n",
    "    dataset = datasets_ehr[k]()\n",
    "    generate_graphs_on_latest_model(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in datasets_ehr :\n",
    "    dataset = datasets_ehr[k]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in datasets_ehr :\n",
    "    dataset = datasets_ehr[k]()\n",
    "    train_dataset(dataset, 'logodds_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in datasets_ehr :\n",
    "    dataset = datasets_ehr[k]()\n",
    "    run_evaluator_on_latest_model(dataset, 'logodds_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.LR import LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in datasets_ehr :\n",
    "    if k != 'pheno' : continue\n",
    "    dataset = datasets_ehr[k]()\n",
    "    train_lr_on_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in datasets_ehr :\n",
    "    dataset = datasets_ehr[k]()\n",
    "    push_all_models(dataset, dataset.keys_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in datasets_ehr :\n",
    "    dataset = datasets_ehr[k]()\n",
    "    run_logodds_experiment(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(enumerate(dataset.vec.label_headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_words(dataset, config='lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(dataset.test_data.yt_hat)\n",
    "idx_y = np.where(y > 0.8)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "top_words_dict = defaultdict(float)\n",
    "for i in idx_y :\n",
    "    d = dataset.test_data.top_words_attn[i]\n",
    "    for k, v in d.items() :\n",
    "        top_words_dict[k] += v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = dict(sorted(top_words_dict.items(), key=lambda x: x[1])[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.print_all_features(n=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_attn = set(top_words.keys())\n",
    "top_words_lr = set(lr.get_features(n=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_attn & top_words_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(top_words_attn & top_words_lr) / len(top_words_attn | top_words_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.keys_to_use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi Adversarial Examples\n",
    "=========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in datasets_ehr :\n",
    "    dataset = datasets_ehr[k]()\n",
    "#     generate_adversarial_examples(dataset, config='lstm')\n",
    "#     generate_logodds_examples(dataset, config='lstm')\n",
    "#     generate_graphs_on_latest_model(dataset, config='lstm')\n",
    "    push_all_models(dataset, keys=dataset.keys_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.vec.label_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in datasets_ehr :\n",
    "    dataset = datasets_ehr[k]()\n",
    "    dataset.display_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.test_data.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = run_evaluator_on_latest_model(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_adversarial_outputs = pload(evaluator.model, 'multi_adversarial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Trainers.PlottingBC import *\n",
    "test_data = dataset.test_data\n",
    "emax_jds, emax_adv_attn, emax_ad_y = plot_multi_adversarial(test_data.X, test_data.yt_hat, \n",
    "                                                            test_data.attn_hat, multi_adversarial_outputs, dirname=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_adversarial_examples(dataset, test_data.X, test_data.yt_hat, test_data.attn_hat, \n",
    "                           emax_jds, emax_adv_attn, emax_ad_y, by_class=None, dirname='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.random.randn(100, 10) + 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.mean(0).min(), a.min(0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
